"""
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Apache Spark –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
"""

import os
import platform
from pyspark.sql import SparkSession
from typing import Dict, Optional

# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è Windows: –æ–±—Ö–æ–¥ –ø—Ä–æ–±–ª–µ–º—ã —Å Hadoop NativeIO
if platform.system() == "Windows":
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å winutils
    os.environ['HADOOP_HOME'] = os.environ.get('HADOOP_HOME', '')
    # –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É NativeIO –Ω–∞ Windows
    os.environ['HADOOP_OPTS'] = os.environ.get('HADOOP_OPTS', '') + ' -Djava.library.path='
    # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ native-hadoop library (—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è Windows)
    import warnings
    import logging
    # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è PySpark –æ native-hadoop
    logging.getLogger("py4j").setLevel(logging.ERROR)
    # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ native library —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'


class SparkConfig:
    """–ö–ª–∞—Å—Å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏—è Spark —Å–µ—Å—Å–∏–∏"""
    
    # JDBC –¥—Ä–∞–π–≤–µ—Ä –¥–ª—è PostgreSQL (–±—É–¥–µ—Ç —Å–∫–∞—á–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
    POSTGRESQL_DRIVER = "org.postgresql.Driver"
    POSTGRESQL_JAR_PACKAGE = "org.postgresql:postgresql:42.7.1"
    
    def __init__(self, app_name: str = "SportsStatsAnalysis"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Spark
        
        Args:
            app_name: –ù–∞–∑–≤–∞–Ω–∏–µ Spark –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        """
        self.app_name = app_name
        self.spark_session: Optional[SparkSession] = None

    def _setup_environment(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Windows"""
        import sys

        if sys.platform == 'win32':
            print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è Windows –¥–ª—è Spark...")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Java
            if 'JAVA_HOME' not in os.environ:
                print("‚ö†Ô∏è JAVA_HOME –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è.")
                print("   –ò–ª–∏ –¥–æ–±–∞–≤—å—Ç–µ: set JAVA_HOME=C:\\Program Files\\Java\\jdk-17")
                print("   –ó–∞—Ç–µ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω–∞–ª/IDE.")
            else:
                java_home = os.environ['JAVA_HOME']
                print(f"‚úÖ JAVA_HOME: {java_home}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ java.exe —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                java_exe = os.path.join(java_home, 'bin', 'java.exe')
                if not os.path.exists(java_exe):
                    print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: java.exe –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {os.path.join(java_home, 'bin')}")
                    print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—É—Ç–∏ JAVA_HOME: {java_home}")
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º Java –≤ PATH –µ—Å–ª–∏ –µ—â–µ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞
                    java_bin = os.path.join(java_home, 'bin')
                    current_path = os.environ.get('PATH', '')
                    if java_bin not in current_path:
                        os.environ['PATH'] = java_bin + ";" + current_path
                        print(f"‚úÖ Java –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ PATH: {java_bin}")

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            temp_dirs = [
                "C:/temp/spark-warehouse",
                "C:/temp/spark-events",
                "C:/temp/spark/tmp",
                "C:/temp/hadoop/tmp"
            ]

            for temp_dir in temp_dirs:
                try:
                    os.makedirs(temp_dir, exist_ok=True)
                except Exception as e:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {temp_dir}: {e}")

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Hadoop/Spark –Ω–∞ Windows
            os.environ['HADOOP_HOME'] = os.environ.get('HADOOP_HOME', '')
            os.environ['HADOOP_OPTS'] = os.environ.get('HADOOP_OPTS', '') + ' -Djava.library.path='
            os.environ['SPARK_LOCAL_DIRS'] = 'C:/temp/spark/tmp'
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PySpark –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            os.environ['PYSPARK_PYTHON'] = sys.executable
            os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

    def _verify_java_setup(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É Java –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º Spark —Å–µ—Å—Å–∏–∏"""
        import sys
        import subprocess
        
        if sys.platform == 'win32':
            if 'JAVA_HOME' not in os.environ:
                raise RuntimeError(
                    "JAVA_HOME –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Java JDK –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è JAVA_HOME.\n"
                    "–ù–∞–ø—Ä–∏–º–µ—Ä: set JAVA_HOME=C:\\Program Files\\Java\\jdk-17"
                )
            
            java_home = os.environ['JAVA_HOME']
            java_exe = os.path.join(java_home, 'bin', 'java.exe')
            
            if not os.path.exists(java_exe):
                raise RuntimeError(
                    f"java.exe –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {os.path.join(java_home, 'bin')}\n"
                    f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—É—Ç–∏ JAVA_HOME: {java_home}"
                )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ Java —Ä–∞–±–æ—Ç–∞–µ—Ç
            # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: java -version –≤—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ stderr, –ø–æ—ç—Ç–æ–º—É –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º stderr –≤ stdout
            try:
                result = subprocess.run(
                    [java_exe, '-version'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    timeout=5
                )
                if result.returncode != 0:
                    raise RuntimeError(f"Java –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ. –ö–æ–¥ –≤–æ–∑–≤—Ä–∞—Ç–∞: {result.returncode}")
            except subprocess.TimeoutExpired:
                raise RuntimeError("–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Java. Java –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
            except Exception as e:
                raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Java: {e}")

    def create_spark_session(
            self,
            master: str = "local[*]",
            memory: str = "4g",
            **kwargs
    ) -> SparkSession:
        # –î–ª—è Windows –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if platform.system() == "Windows":
            if master == "local[*]":
                master = "local[1]"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 1 —è–¥—Ä–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            if memory in ["4g", "2g"]:
                memory = "1g"  # –£–º–µ–Ω—å—à–∞–µ–º –ø–∞–º—è—Ç—å –¥–ª—è Windows –¥–æ –º–∏–Ω–∏–º—É–º–∞
        """
        –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Spark —Å–µ—Å—Å–∏—é —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

        Args:
            master: Spark master URL (default: local[*] - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞)
            memory: –û–±—ä–µ–º –ø–∞–º—è—Ç–∏ –¥–ª—è –¥—Ä–∞–π–≤–µ—Ä–∞
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

        Returns:
            SparkSession: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è Spark —Å–µ—Å—Å–∏—è
        """
        if self.spark_session is not None:
            return self.spark_session

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ –ü–ï–†–ï–î —Å–æ–∑–¥–∞–Ω–∏–µ–º —Å–µ—Å—Å–∏–∏
        self._setup_environment()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ Java –¥–ª—è Windows (–º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è)
        if platform.system() == "Windows":
            try:
                self._verify_java_setup()
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                if 'JAVA_HOME' in os.environ:
                    java_home = os.environ['JAVA_HOME']
                    java_exe = os.path.join(java_home, 'bin', 'java.exe')
                    if os.path.exists(java_exe):
                        print(f"‚úÖ Java –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é: {java_exe}")
                    else:
                        print(f"‚ö†Ô∏è  java.exe –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {os.path.join(java_home, 'bin')}")
            except RuntimeError as e:
                # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º
                import warnings
                warnings.warn(f"–ü—Ä–æ–±–ª–µ–º–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π Java: {e}. Spark –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.", RuntimeWarning)
                print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {e}")
                print(f"   –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ Java...")

        # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        spark_config = {
            "spark.app.name": self.app_name,
            "spark.master": master,
            "spark.driver.memory": memory,
            "spark.jars.packages": self.POSTGRESQL_JAR_PACKAGE,
            "spark.driver.extraClassPath": self._get_jdbc_driver_path(),

            # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            "spark.sql.execution.arrow.pyspark.enabled": "true",
            "spark.sql.execution.arrow.pyspark.fallback.enabled": "true",
           # "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
           # "spark.kryo.registrator": "org.apache.spark.serializer.KryoRegistrator",
        }
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ-Windows —Å–∏—Å—Ç–µ–º
        if platform.system() != "Windows":
            spark_config.update({
                "spark.sql.adaptive.enabled": "true",
                "spark.sql.adaptive.coalescePartitions.enabled": "true",
            })

        # –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø WINDOWS –ò RDD –û–ü–ï–†–ê–¶–ò–ô
        if platform.system() == "Windows":
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ø–∞–º—è—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
            spark_config["spark.driver.memory"] = memory
            
            spark_config.update({
                # –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Windows
                "spark.python.worker.reuse": "false",  # –û—Ç–∫–ª—é—á–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–æ—Ä–∫–µ—Ä–æ–≤
                "spark.python.worker.timeout": "600",  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –≤–æ—Ä–∫–µ—Ä–æ–≤ (—Å–µ–∫—É–Ω–¥—ã)
                "spark.executor.heartbeatInterval": "60s",  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª heartbeat
                "spark.network.timeout": "900s",  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å–µ—Ç–µ–≤–æ–π —Ç–∞–π–º–∞—É—Ç
                "spark.rpc.message.maxSize": "512",  # –£–º–µ–Ω—å—à–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π
                "spark.rpc.askTimeout": "600s",  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç RPC
                "spark.rpc.lookupTimeout": "300s",  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –ø–æ–∏—Å–∫–∞ RPC
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ Java gateway –Ω–∞ Windows
                "spark.driver.host": "localhost",
                "spark.driver.bindAddress": "127.0.0.1",
                "spark.driver.port": "0",  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø–æ—Ä—Ç–∞
                "spark.blockManager.port": "0",  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø–æ—Ä—Ç–∞
                "spark.ui.port": "0",  # –û—Ç–∫–ª—é—á–∞–µ–º UI –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
                "spark.ui.enabled": "false",  # –û—Ç–∫–ª—é—á–∞–µ–º UI
                
                # –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Java gateway - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
                "spark.driver.maxResultSize": "512m",  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ GC (–±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–∞–º—è—Ç–∏ - –æ–Ω–∏ –∑–∞–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ spark.driver.memory)
                "spark.driver.extraJavaOptions": "-XX:+UseG1GC -XX:MaxGCPauseMillis=200",
                # –î–ª—è executor –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º extraJavaOptions, —Ç–∞–∫ –∫–∞–∫ –≤ local —Ä–µ–∂–∏–º–µ executor –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                
                # –û—Ç–∫–ª—é—á–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
                "spark.sql.adaptive.enabled": "false",  # –û—Ç–∫–ª—é—á–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
                "spark.sql.adaptive.coalescePartitions.enabled": "false",
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π Windows
                "spark.sql.warehouse.dir": "file:///C:/temp/spark-warehouse",
                "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version": "2",
                "spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored": "true",
                "spark.cleaner.referenceTracking.cleanCheckpoints": "false",  # –û—Ç–∫–ª—é—á–∞–µ–º –æ—á–∏—Å—Ç–∫—É checkpoint
                "spark.local.dir": "C:/temp/spark/tmp",  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Windows
                "spark.io.compression.codec": "snappy",
                "spark.shuffle.compress": "true",
                "spark.shuffle.spill.compress": "true",
            })
        else:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Linux/Mac
            spark_config.update({
                "spark.python.worker.reuse": "true",
                "spark.python.worker.timeout": "60",
                "spark.network.timeout": "120s",
            })

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è RDD –æ–ø–µ—Ä–∞—Ü–∏–π (–æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω—ã –¥–ª—è –∑–∞–¥–∞—á–∏ 4)
        spark_config.update({
            "spark.default.parallelism": "4",  # –£–º–µ–Ω—å—à–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–ª—è Windows
            "spark.sql.shuffle.partitions": "4",  # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π
            "spark.rdd.compress": "true",  # –°–∂–∏–º–∞–µ–º RDD –¥–∞–Ω–Ω—ã–µ
            "spark.shuffle.file.buffer": "1mb",  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±—É—Ñ–µ—Ä shuffle
            "spark.reducer.maxSizeInFlight": "96mb",  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –≤ –ø–æ–ª–µ—Ç–µ
        })

        # –°–æ–∑–¥–∞–µ–º builder —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        builder = SparkSession.builder

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        for key, value in spark_config.items():
            builder = builder.config(key, value)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ kwargs
        for key, value in kwargs.items():
            builder = builder.config(key, value)

        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ native-hadoop library –Ω–∞ Windows
            if platform.system() == "Windows":
                import warnings
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore", message=".*native-hadoop.*")
                    warnings.filterwarnings("ignore", message=".*Unable to load native-hadoop.*")
                    self.spark_session = builder.getOrCreate()
            else:
                self.spark_session = builder.getOrCreate()
            
            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —É—Ä–æ–≤–Ω—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            self.spark_session.sparkContext.setLogLevel("WARN")
            # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ native library –≤ –ª–æ–≥–∞—Ö Spark
            if platform.system() == "Windows":
                import logging
                logging.getLogger("org.apache.spark").setLevel(logging.ERROR)
                logging.getLogger("org.apache.hadoop").setLevel(logging.ERROR)
            print("‚úÖ Spark —Å–µ—Å—Å–∏—è —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞")
        except Exception as e:
            error_msg = str(e)
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å —Ç–µ–∫—É—â–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏, –ø—Ä–æ–±—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if platform.system() == "Windows" and ("JAVA_GATEWAY_EXITED" in error_msg or "Java gateway" in error_msg):
                print("\n‚ö†Ô∏è  –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å Spark —Å–µ—Å—Å–∏—é —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏...")
                print("   (–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥)")
                try:
                    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Windows - –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –º–∏–Ω–∏–º—É–º
                    minimal_config = {
                        "spark.app.name": self.app_name,
                        "spark.master": "local[1]",
                        "spark.driver.memory": "512m",
                        "spark.driver.maxResultSize": "256m",
                        "spark.ui.enabled": "false",
                        "spark.sql.adaptive.enabled": "false",
                        "spark.jars.packages": self.POSTGRESQL_JAR_PACKAGE,
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        "spark.driver.host": "127.0.0.1",
                        "spark.driver.bindAddress": "127.0.0.1",
                        "spark.driver.port": "0",
                        "spark.blockManager.port": "0",
                        "spark.network.timeout": "600s",
                        "spark.python.worker.reuse": "false",
                        "spark.python.worker.timeout": "300",
                    }
                    minimal_builder = SparkSession.builder
                    for key, value in minimal_config.items():
                        minimal_builder = minimal_builder.config(key, value)
                    
                    # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Å–µ—Å—Å–∏–∏
                    import warnings
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        self.spark_session = minimal_builder.getOrCreate()
                    
                    self.spark_session.sparkContext.setLogLevel("ERROR")  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    print("‚úÖ Spark —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")
                except Exception as e2:
                    # –ï—Å–ª–∏ –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –æ—à–∏–±–∫—É —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
                    print(f"\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å Spark —Å–µ—Å—Å–∏—é –¥–∞–∂–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")
                    print(f"   –û—à–∏–±–∫–∞: {error_msg}")
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–¥ –≤—ã–±—Ä–æ—Å–æ–º –æ—à–∏–±–∫–∏
                    print("\nüîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:")
                    if 'JAVA_HOME' in os.environ:
                        java_home = os.environ['JAVA_HOME']
                        java_exe = os.path.join(java_home, 'bin', 'java.exe')
                        print(f"   JAVA_HOME: {java_home}")
                        print(f"   java.exe —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(java_exe)}")
                        if os.path.exists(java_exe):
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é Java
                            try:
                                import subprocess
                                result = subprocess.run(
                                    [java_exe, '-version'],
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.STDOUT,
                                    text=True,
                                    timeout=5
                                )
                                if result.returncode == 0:
                                    version = result.stdout.split('\n')[0] if result.stdout else "Unknown"
                                    print(f"   Java –≤–µ—Ä—Å–∏—è: {version.strip()}")
                            except:
                                pass
                    else:
                        print("   ‚ö†Ô∏è  JAVA_HOME –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
                    print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
                    print("   1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Java JDK 8, 11, 17 –∏–ª–∏ 21 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                    print("   2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è JAVA_HOME")
                    print("   3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Ä–º–∏–Ω–∞–ª/IDE –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Java")
                    print("   4. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å –ø–∞–º—è—Ç—å: --memory 1g")
                    print("   5. –ó–∞–∫—Ä–æ–π—Ç–µ –¥—Ä—É–≥–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ Java")
                    raise e
            else:
                raise

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è Windows –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏
        if platform.system() == "Windows":
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏
            import sys
            os.environ['PYSPARK_PYTHON'] = sys.executable
            os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
            temp_dirs = [
                "C:/temp/spark-warehouse",
                "C:/temp/spark-events",
                "C:/temp/hadoop/tmp"
            ]

            for temp_dir in temp_dirs:
                os.makedirs(temp_dir, exist_ok=True)

        return self.spark_session

    def create_spark_session_for_rdd(
            self,
            master: str = "local[2]",  # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–¥–µ—Ä –¥–ª—è RDD –æ–ø–µ—Ä–∞—Ü–∏–π
            memory: str = "2g",  # –£–º–µ–Ω—å—à–∞–µ–º –ø–∞–º—è—Ç—å –¥–ª—è RDD
            **kwargs
    ) -> SparkSession:
        """
        –°–æ–∑–¥–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é Spark —Å–µ—Å—Å–∏—é –¥–ª—è RDD –æ–ø–µ—Ä–∞—Ü–∏–π (–ó–∞–¥–∞—á–∞ 4)

        Args:
            master: Spark master URL (–º–µ–Ω—å—à–µ —è–¥–µ—Ä –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
            memory: –û–±—ä–µ–º –ø–∞–º—è—Ç–∏ –¥–ª—è –¥—Ä–∞–π–≤–µ—Ä–∞
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

        Returns:
            SparkSession: –°–µ—Å—Å–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è RDD
        """
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è RDD
        rdd_kwargs = {
            "spark.python.worker.reuse": "false",
            "spark.python.worker.timeout": "600",  # –ï—â–µ –±–æ–ª—å—à–µ –¥–ª—è RDD
            "spark.network.timeout": "900s",  # –ë–æ–ª—å—à–µ —Ç–∞–π–º–∞—É—Ç
            "spark.default.parallelism": "2",  # –ú–∏–Ω–∏–º—É–º –ø–∞—Ä—Ç–∏—Ü–∏–π
            "spark.sql.shuffle.partitions": "2",  # –ú–∏–Ω–∏–º—É–º –ø–∞—Ä—Ç–∏—Ü–∏–π
            "spark.locality.wait": "3s",  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
        }

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        rdd_kwargs.update(kwargs)

        return self.create_spark_session(master=master, memory=memory, **rdd_kwargs)

    def configure_for_windows_rdd(self):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è RDD –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ Windows
        –í—ã–∑—ã–≤–∞–π—Ç–µ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º RDD –æ–ø–µ—Ä–∞—Ü–∏–π
        """
        if platform.system() == "Windows" and self.spark_session is not None:
            try:
                # –ù–µ –≤—Å–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏
                # –≠—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ create_spark_session
                pass
            except Exception as e:
                import warnings
                warnings.warn(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤—Å–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ RDD: {e}")
    
    def _get_jdbc_driver_path(self) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—É—Ç—å –∫ JDBC –¥—Ä–∞–π–≤–µ—Ä—É PostgreSQL
        
        Returns:
            str: –ü—É—Ç—å –∫ –¥—Ä–∞–π–≤–µ—Ä—É –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        """
        # Spark –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∞–µ—Ç –¥—Ä–∞–π–≤–µ—Ä —á–µ—Ä–µ–∑ maven
        return ""
    
    def get_postgres_jdbc_url(self, db_config: Dict[str, str]) -> str:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç JDBC URL –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL
        
        Args:
            db_config: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
                      (host, port, database, user, password)
        
        Returns:
            str: JDBC URL
        """
        host = db_config.get('host', 'localhost')
        port = db_config.get('port', 5432)
        database = db_config.get('database', 'sports_stats')
        
        return f"jdbc:postgresql://{host}:{port}/{database}"
    
    def get_jdbc_properties(self, db_config: Dict[str, str]) -> Dict[str, str]:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç properties –¥–ª—è JDBC –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        
        Args:
            db_config: –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        
        Returns:
            Dict: Properties –¥–ª—è JDBC
        """
        return {
            "user": db_config.get('user', 'postgres'),
            "password": db_config.get('password', 'postgres'),
            "driver": self.POSTGRESQL_DRIVER
        }
    
    def stop_spark_session(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç Spark —Å–µ—Å—Å–∏—é"""
        if self.spark_session is not None:
            try:
                # –ù–∞ Windows –ø–æ–¥–∞–≤–ª—è–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                if platform.system() == "Windows":
                    import warnings
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        # –ü—ã—Ç–∞–µ–º—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Å—Å–∏—é
                        try:
                            self.spark_session.stop()
                        except Exception as e:
                            error_msg = str(e).lower()
                            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —É–¥–∞–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–∞ Windows
                            if any(keyword in error_msg for keyword in [
                                "delete", "temp", "temporary", "cleanup", 
                                "unable to delete", "exception while deleting"
                            ]):
                                # –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è Windows, –ø—Ä–æ—Å—Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
                                pass
                            else:
                                # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ –ª–æ–≥–∏—Ä—É–µ–º, –Ω–æ –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                                import logging
                                logger = logging.getLogger(__name__)
                                logger.debug(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ Spark —Å–µ—Å—Å–∏–∏: {e}")
                else:
                    # –î–ª—è –Ω–µ-Windows —Å–∏—Å—Ç–µ–º –æ–±—ã—á–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
                    self.spark_session.stop()
            except Exception as e:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ (–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã)
                import logging
                logger = logging.getLogger(__name__)
                logger.debug(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ Spark —Å–µ—Å—Å–∏–∏: {e}")
            finally:
                self.spark_session = None
    
    def __enter__(self):
        """Context manager: —Å–æ–∑–¥–∞–µ—Ç —Å–µ—Å—Å–∏—é –ø—Ä–∏ –≤—Ö–æ–¥–µ"""
        return self.create_spark_session()
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager: –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ"""
        self.stop_spark_session()

