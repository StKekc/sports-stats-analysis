"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ Apache Spark SQL
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import pandas as pd
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from .spark_config import SparkConfig
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from scipy.stats import mstats

logger = logging.getLogger(__name__)

class SparkProcessor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Apache Spark
    –ß–∏—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ PostgreSQL, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Spark SQL
    """
    def __init__(self, db_config: Dict[str, str], spark_config: Optional[SparkConfig] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            db_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL
            spark_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Spark (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        """
        self.db_config = db_config
        self.spark_config = spark_config or SparkConfig()
        self.spark: Optional[SparkSession] = None
        self.jdbc_url = self.spark_config.get_postgres_jdbc_url(db_config)
        self.jdbc_properties = self.spark_config.get_jdbc_properties(db_config)
    
    def initialize_spark(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Spark —Å–µ—Å—Å–∏—é"""
        if self.spark is None:
            logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark —Å–µ—Å—Å–∏–∏...")
            self.spark = self.spark_config.create_spark_session()
            logger.info("‚úÖ Spark —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞")
    
    def read_table_from_postgres(self, table_name: str) -> DataFrame:
        """
        –ß–∏—Ç–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É –∏–∑ PostgreSQL –≤ Spark DataFrame
        
        Args:
            table_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        
        Returns:
            DataFrame: Spark DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–∞–±–ª–∏—Ü—ã
        """
        self.initialize_spark()
        
        logger.info(f"–ß—Ç–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã '{table_name}' –∏–∑ PostgreSQL...")
        
        df = self.spark.read.jdbc(
            url=self.jdbc_url,
            table=table_name,
            properties=self.jdbc_properties
        )
        
        count = df.count()
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}'")
        
        return df
    
    def read_query_from_postgres(self, query: str) -> DataFrame:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç SQL –∑–∞–ø—Ä–æ—Å –∫ PostgreSQL –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ Spark DataFrame
        
        Args:
            query: SQL –∑–∞–ø—Ä–æ—Å
        
        Returns:
            DataFrame: Spark DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∑–∞–ø—Ä–æ—Å–∞
        """
        self.initialize_spark()
        
        logger.info(f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ PostgreSQL...")
        
        df = self.spark.read.jdbc(
            url=self.jdbc_url,
            table=f"({query}) as query",
            properties=self.jdbc_properties
        )
        
        return df


    def analyze_team_playing_styles(
            self,
            league_filter: Optional[str] = None,
            season_filter: Optional[str] = None,
            min_matches: int = 10,
            n_clusters: Optional[int] = None
    ) -> pd.DataFrame:
        """
        –ó–ê–î–ê–ß–ê 1: –ê–Ω–∞–ª–∏–∑ –∏–≥—Ä–æ–≤—ã—Ö —Å—Ç–∏–ª–µ–π –∫–æ–º–∞–Ω–¥ —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–º–∞–Ω–¥ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏—Ö –∏–≥—Ä–æ–≤—ã–µ —Å—Ç–∏–ª–∏
        —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ K-means –Ω–∞ –æ—Å–Ω–æ–≤–µ Spark SQL.
        Args:
            league_filter: –§–∏–ª—å—Ç—Ä –ø–æ –ª–∏–≥–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'epl')
            season_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∑–æ–Ω—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, '2023-2024')
            min_matches: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞—Ç—á–µ–π –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã
            n_clusters: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–µ—Å–ª–∏ None, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        Returns:
            pd.DataFrame: DataFrame —Å –∫–æ–º–∞–Ω–¥–∞–º–∏ –∏ –∏—Ö –∏–≥—Ä–æ–≤—ã–º–∏ —Å—Ç–∏–ª—è–º–∏
        """
        self.initialize_spark()

        logger.info("=" * 70)
        logger.info("–ó–ê–î–ê–ß–ê 1: –ê–Ω–∞–ª–∏–∑ –∏–≥—Ä–æ–≤—ã—Ö —Å—Ç–∏–ª–µ–π –∫–æ–º–∞–Ω–¥ —á–µ—Ä–µ–∑ Spark –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é")
        logger.info("=" * 70)

        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ Spark SQL
        logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–º–∞–Ω–¥...")
        prepared_data = self._prepare_team_style_data(
            league_filter, season_filter, min_matches
        )

        # 2. –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        logger.info("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∏–ª–µ–π...")
        metrics_df = self._create_style_metrics(prepared_data)

        # 3. –í—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
        logger.info("üîç –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ K-means...")
        clustering_result = self._perform_team_clustering(metrics_df, n_clusters)

        # 4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
        logger.info("üìà –ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
        final_result = self._analyze_cluster_styles(clustering_result)

        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        self._save_style_analysis_results(final_result)

        logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ {final_result['n_clusters']} —Å—Ç–∏–ª–µ–π –∏–≥—Ä—ã")

        return final_result['teams_with_styles']

    def _prepare_team_style_data(
            self,
            league_filter: Optional[str],
            season_filter: Optional[str],
            min_matches: int
    ) -> DataFrame:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–º–∞–Ω–¥ —á–µ—Ä–µ–∑ Spark SQL
        Returns:
            DataFrame: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥
        """
        # –ß–∏—Ç–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –∏–∑ PostgreSQL
        team_stats_df = self.read_table_from_postgres("team_season_stats")
        teams_df = self.read_table_from_postgres("teams")
        leagues_df = self.read_table_from_postgres("leagues")
        seasons_df = self.read_table_from_postgres("seasons")

        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è Spark SQL
        team_stats_df.createOrReplaceTempView("team_season_stats")
        teams_df.createOrReplaceTempView("teams")
        leagues_df.createOrReplaceTempView("leagues")
        seasons_df.createOrReplaceTempView("seasons")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —É—Å–ª–æ–≤–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        where_conditions = ["tss.matches_played > 0"]

        if league_filter:
            where_conditions.append(f"l.league_code = '{league_filter}'")

        if season_filter:
            where_conditions.append(f"s.season_code = '{season_filter}'")

        where_clause = " AND ".join(where_conditions)

        # SQL –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        sql_query = f"""
        WITH team_stats_clean AS (
            SELECT 
                tss.*,
                t.team_name,
                l.league_name,
                l.league_code,
                s.season_code,
                -- –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                COALESCE(tss.goals_per_90, 0) as goals_per_90_clean,
                COALESCE(tss.xg_per_90, 0.1) as xg_per_90_clean,
                COALESCE(tss.assists_per_90, 0) as assists_per_90_clean,
                COALESCE(tss.possession_pct, 0) as possession_pct_clean,
                COALESCE(tss.progressive_passes, 0) as progressive_passes_clean,
                COALESCE(tss.yellow_cards, 0) as yellow_cards_clean,
                COALESCE(tss.avg_age, 25.0) as avg_age_clean,
                COALESCE(tss.players_used, 20) as players_used_clean
            FROM team_season_stats tss
            LEFT JOIN teams t ON tss.team_id = t.team_id
            LEFT JOIN leagues l ON tss.league_id = l.league_id
            LEFT JOIN seasons s ON tss.season_id = s.season_id
            WHERE {where_clause} 
                AND tss.matches_played >= {min_matches}
                AND tss.minutes > 0
        )
        SELECT 
            team_id,
            team_name,
            league_name,
            league_code,
            season_code,
            season_id,
            matches_played,
            minutes,
            -- –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            goals_per_90_clean as goals_per_90,
            xg_per_90_clean as xg_per_90,
            assists_per_90_clean as assists_per_90,
            possession_pct_clean as possession_pct,
            progressive_passes_clean as progressive_passes,
            yellow_cards_clean as yellow_cards,
            avg_age_clean as avg_age,
            players_used_clean as players_used
        FROM team_stats_clean
        """

        logger.info(f"–§–∏–ª—å—Ç—Ä—ã: –ª–∏–≥–∞={league_filter or '–≤—Å–µ'}, "
                    f"—Å–µ–∑–æ–Ω={season_filter or '–≤—Å–µ'}, –º–∏–Ω.–º–∞—Ç—á–µ–π={min_matches}")

        result_df = self.spark.sql(sql_query)
        count = result_df.count()
        logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–º–∞–Ω–¥")

        return result_df

    def _create_style_metrics(self, team_data: DataFrame) -> DataFrame:
        """
        –°–æ–∑–¥–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–≥—Ä–æ–≤—ã—Ö —Å—Ç–∏–ª–µ–π
        Args:
            team_data: DataFrame —Å –±–∞–∑–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        Returns:
            DataFrame: DataFrame —Å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å—Ç–∏–ª—è
        """
        from pyspark.sql import functions as F

        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏–≥—Ä–æ–≤–æ–≥–æ —Å—Ç–∏–ª—è...")

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics_df = team_data.select(
            F.col("team_id"),
            F.col("team_name"),
            F.col("league_name"),
            F.col("season_code"),
            F.col("season_id"),

            # 1. –ê—Ç–∞–∫—É—é—â–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª (–±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏)
            F.col("goals_per_90").alias("attacking_power"),
            F.col("xg_per_90").alias("expected_attacking"),

            # 2. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞—Ç–∞–∫–∏ (–≥–æ–ª—ã / xG)
            (F.col("goals_per_90") /
             F.when(F.col("xg_per_90") > 0.1, F.col("xg_per_90"))
             .otherwise(0.1)).alias("attack_efficiency"),

            # 3. –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–º–µ–Ω—Ç–æ–≤
            F.col("assists_per_90").alias("creativity"),
            F.col("progressive_passes").alias("progressive_actions"),

            # 4. –ö–æ–Ω—Ç—Ä–æ–ª—å –∏ –≤–ª–∞–¥–µ–Ω–∏–µ –º—è—á–æ–º
            F.col("possession_pct").alias("possession_control"),

            # 5. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –∏–≥—Ä–∞
            F.col("yellow_cards").alias("aggressiveness"),

            # 6. –í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
            F.col("avg_age").alias("team_age_profile"),

            # 7. –®–∏—Ä–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ—Å—Ç–∞–≤–∞
            F.col("players_used").alias("squad_rotation"),

            # 8. –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∞—Ç–∞–∫–∏ (–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –≥–æ–ª)
            (F.col("progressive_passes") /
             F.when(F.col("goals_per_90") > 0.5, F.col("goals_per_90"))
             .otherwise(0.5)).alias("attack_variety"),

            # 9. –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –∞—Ç–∞–∫–∏ (–≥–æ–ª—ã –Ω–∞ –≤–ª–∞–¥–µ–Ω–∏–µ)
            (F.col("goals_per_90") * 100 /
             F.when(F.col("possession_pct") > 10, F.col("possession_pct"))
             .otherwise(10)).alias("attack_intensity")
        )

        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        metrics_df = metrics_df.filter(
            (F.col("attacking_power") >= 0) &
            (F.col("possession_control") >= 0) &
            (F.col("possession_control") <= 100) &
            (F.col("team_age_profile") >= 18) &
            (F.col("team_age_profile") <= 40)
        )

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ–ø—É—Å–∫–∏
        metrics_df = metrics_df.fillna(0)

        final_count = metrics_df.count()
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {final_count} –∑–∞–ø–∏—Å–µ–π —Å {len(metrics_df.columns)} –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å—Ç–∏–ª—è")

        return metrics_df

    def _perform_team_clustering(
            self,
            metrics_df: DataFrame,
            n_clusters: Optional[int] = None
    ) -> Dict:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∫–æ–º–∞–Ω–¥ –ø–æ —Å—Ç–∏–ª—è–º –∏–≥—Ä—ã
        Args:
            metrics_df: DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å—Ç–∏–ª—è
            n_clusters: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (None –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        """
        logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ K-means...")

        # –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        feature_columns = [
            "attacking_power",
            "attack_efficiency",
            "creativity",
            "possession_control",
            "aggressiveness",
            "team_age_profile",
            "squad_rotation",
            "attack_variety",
            "attack_intensity"
        ]

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ pandas –¥–ª—è scikit-learn
        logger.info("–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è Spark DataFrame –≤ pandas –¥–ª—è ML...")
        pandas_df = metrics_df.select(
            "team_id", "team_name", "league_name", "season_code", *feature_columns
        ).toPandas()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö
        if len(pandas_df) < 10:
            logger.warning(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {len(pandas_df)} –∑–∞–ø–∏—Å–µ–π")
            return {"data": pandas_df, "labels": np.zeros(len(pandas_df), dtype=int)}

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        X = pandas_df[feature_columns].values

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        if n_clusters is None:
            optimal_k = self._find_optimal_cluster_count(X_scaled)
            n_clusters = optimal_k
            logger.info(f"–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ = {n_clusters}")

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        n_clusters = min(n_clusters, max(2, len(pandas_df) // 10))

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é K-means
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score

        kmeans = KMeans(
            n_clusters=n_clusters,
            random_state=42,
            n_init=20,
            max_iter=300
        )

        cluster_labels = kmeans.fit_predict(X_scaled)

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        if n_clusters > 1:
            silhouette_avg = silhouette_score(X_scaled, cluster_labels)
        else:
            silhouette_avg = 0

        logger.info(f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        logger.info(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
        logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Å–∏–ª—É—ç—Ç–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: {silhouette_avg:.3f}")
        logger.info(f"  ‚Ä¢ Inertia: {kmeans.inertia_:.2f}")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
        for cluster_id, count in cluster_counts.items():
            logger.info(f"  ‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {count} –∫–æ–º–∞–Ω–¥ ({count / len(pandas_df) * 100:.1f}%)")

        return {
            "data": pandas_df,
            "features": feature_columns,
            "labels": cluster_labels,
            "centers": kmeans.cluster_centers_,
            "scaler": scaler,
            "model": kmeans,
            "silhouette_score": silhouette_avg,
            "n_clusters": n_clusters,
            "cluster_counts": cluster_counts.to_dict()
        }

    def _find_optimal_cluster_count(self, X_scaled: np.ndarray, max_k: int = 8) -> int:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        Args:
            X_scaled: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            max_k: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        Returns:
            int: –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score

        logger.info("–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")

        n_samples = len(X_scaled)
        max_k = min(max_k, n_samples // 3)  # –ù–µ –±–æ–ª—å—à–µ —á–µ–º n_samples/3

        if max_k < 2:
            return 2

        inertia_values = []
        silhouette_values = []
        k_range = range(2, max_k + 1)

        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X_scaled)

            inertia_values.append(kmeans.inertia_)

            if len(set(labels)) > 1:  # –ù–µ –≤—ã—á–∏—Å–ª—è–µ–º —Å–∏–ª—É—ç—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
                silhouette_values.append(silhouette_score(X_scaled, labels))
            else:
                silhouette_values.append(0)

        # –ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è: –Ω–∞—Ö–æ–¥–∏–º —Ç–æ—á–∫—É –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ –∏–∑–≥–∏–±–∞
        if len(inertia_values) > 1:
            diffs = np.diff(inertia_values)
            diff_ratios = diffs[1:] / diffs[:-1] if len(diffs) > 1 else [0]

            if len(diff_ratios) > 0:
                elbow_k = np.argmax(diff_ratios) + 3  # +3 –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ–º —Å k=2
            else:
                elbow_k = 2
        else:
            elbow_k = 2

        # –ú–µ—Ç–æ–¥ —Å–∏–ª—É—ç—Ç–∞: –≤—ã–±–∏—Ä–∞–µ–º k —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å–∏–ª—É—ç—Ç–æ–º
        if len(silhouette_values) > 0:
            silhouette_k = k_range[np.argmax(silhouette_values)]
        else:
            silhouette_k = 2

        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ–±–∞ –º–µ—Ç–æ–¥–∞
        optimal_k = max(elbow_k, silhouette_k, 3)
        optimal_k = min(optimal_k, max_k)

        logger.info(f"–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {optimal_k}")
        logger.info(f"  ‚Ä¢ –ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è: {elbow_k}")
        logger.info(f"  ‚Ä¢ –ú–µ—Ç–æ–¥ —Å–∏–ª—É—ç—Ç–∞: {silhouette_k}")

        return optimal_k

    def _analyze_cluster_styles(self, clustering_result: Dict) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏–≥—Ä–æ–≤—ã–µ —Å—Ç–∏–ª–∏
        Args:
            clustering_result: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ —Å—Ç–∏–ª—è–º–∏
        """
        logger.info("–ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")

        pandas_df = clustering_result["data"]
        cluster_labels = clustering_result["labels"]
        cluster_centers = clustering_result["centers"]
        n_clusters = clustering_result["n_clusters"]
        scaler = clustering_result["scaler"]

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ –¥–∞–Ω–Ω—ã–µ
        pandas_df["cluster"] = cluster_labels

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä
        cluster_analysis = []
        style_mapping = {}

        for cluster_id in range(n_clusters):
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_data = pandas_df[pandas_df["cluster"] == cluster_id]

            if len(cluster_data) == 0:
                continue

            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
            cluster_means = cluster_data.mean(numeric_only=True)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∏–ª—å –∏–≥—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            style_name = self._determine_playing_style_from_metrics(
                attacking=cluster_means.get("attacking_power", 0),
                possession=cluster_means.get("possession_control", 0),
                efficiency=cluster_means.get("attack_efficiency", 0),
                creativity=cluster_means.get("creativity", 0),
                aggressiveness=cluster_means.get("aggressiveness", 0),
                age=cluster_means.get("team_age_profile", 0)
            )

            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_info = {
                "cluster_id": cluster_id,
                "style_name": style_name,
                "team_count": len(cluster_data),
                "percentage": len(cluster_data) / len(pandas_df) * 100,
                "avg_attacking": cluster_means.get("attacking_power", 0),
                "avg_possession": cluster_means.get("possession_control", 0),
                "avg_efficiency": cluster_means.get("attack_efficiency", 0),
                "avg_creativity": cluster_means.get("creativity", 0),
                "avg_aggressiveness": cluster_means.get("aggressiveness", 0),
                "avg_age": cluster_means.get("team_age_profile", 0),
                "top_teams": cluster_data.nlargest(3, "attacking_power")["team_name"].tolist()
            }

            cluster_analysis.append(cluster_info)
            style_mapping[cluster_id] = style_name

            logger.info(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {style_name}")
            logger.info(f"  ‚Ä¢ –ö–æ–º–∞–Ω–¥: {cluster_info['team_count']} ({cluster_info['percentage']:.1f}%)")
            logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –∞—Ç–∞–∫–∞: {cluster_info['avg_attacking']:.2f}")
            logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤–ª–∞–¥–µ–Ω–∏–µ: {cluster_info['avg_possession']:.1f}%")
            logger.info(f"  ‚Ä¢ –¢–æ–ø –∫–æ–º–∞–Ω–¥—ã: {', '.join(cluster_info['top_teams'])}")

        # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Å—Ç–∏–ª–∏ –∫–æ–º–∞–Ω–¥–∞–º
        pandas_df["playing_style"] = pandas_df["cluster"].map(style_mapping)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ª–∏–≥–∞–º
        league_distribution = pandas_df.groupby(["league_name", "playing_style"]).size().unstack(fill_value=0)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∏–ª–µ–π –ø–æ —Å–µ–∑–æ–Ω–∞–º
        style_changes = self._analyze_style_changes_over_time(pandas_df)

        return {
            "teams_with_styles": pandas_df,
            "cluster_analysis": pd.DataFrame(cluster_analysis),
            "league_distribution": league_distribution,
            "style_changes": style_changes,
            "style_mapping": style_mapping,
            "n_clusters": n_clusters,
            "silhouette_score": clustering_result["silhouette_score"]
        }

    def _determine_playing_style_from_metrics(
            self,
            attacking: float,
            possession: float,
            efficiency: float,
            creativity: float,
            aggressiveness: float,
            age: float
    ) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–≥—Ä–æ–≤–æ–≥–æ —Å—Ç–∏–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫
        Returns:
            str: –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∏–ª—è –∏–≥—Ä—ã
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        is_attacking = attacking > 1.4
        is_possession = possession > 55
        is_efficient = efficiency > 1.05
        is_creative = creativity > 0.8
        is_aggressive = aggressiveness > 70
        is_young = age < 26

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∏–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        if is_attacking:
            if is_possession:
                if is_efficient:
                    return "–î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ"
                else:
                    return "–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—â–∏–µ –∞—Ç–∞–∫—É—é—â–∏–µ"
            else:
                if is_efficient:
                    return "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞—Ç–∞–∫—É—é—â–∏–µ"
                else:
                    return "–ü—Ä—è–º—ã–µ –∞—Ç–∞–∫—É—é—â–∏–µ"
        else:
            if is_possession:
                if is_aggressive:
                    return "–ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä—ã"
                else:
                    return "–ü–∞—Å—Å–∏–≤–Ω—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä—ã"
            else:
                if is_aggressive:
                    return "–ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –æ–±–æ—Ä–æ–Ω–∏—Ç–µ–ª—å–Ω—ã–µ"
                elif is_young:
                    return "–ú–æ–ª–æ–¥—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ"
                else:
                    return "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±–æ—Ä–æ–Ω–∏—Ç–µ–ª—å–Ω—ã–µ"

    def _analyze_style_changes_over_time(self, teams_df: pd.DataFrame) -> pd.DataFrame:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∏–ª–µ–π –∫–æ–º–∞–Ω–¥ –ø–æ —Å–µ–∑–æ–Ω–∞–º
        Args:
            teams_df: DataFrame —Å –∫–æ–º–∞–Ω–¥–∞–º–∏ –∏ —Å—Ç–∏–ª—è–º–∏
        Returns:
            DataFrame: –ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∏–ª–µ–π
        """
        logger.info("–ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∏–ª–µ–π –ø–æ —Å–µ–∑–æ–Ω–∞–º...")

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–∞–Ω–¥–µ –∏ —Å–µ–∑–æ–Ω—É
        teams_sorted = teams_df.sort_values(["team_id", "season_id"])

        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–º–∞–Ω–¥—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–µ–∑–æ–Ω–∞–º–∏
        team_season_counts = teams_sorted.groupby("team_id").size()
        teams_with_multiple = team_season_counts[team_season_counts > 1].index

        if len(teams_with_multiple) == 0:
            logger.info("–ù–µ—Ç –∫–æ–º–∞–Ω–¥ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–µ–∑–æ–Ω–∞–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π")
            return pd.DataFrame()

        style_changes = []

        for team_id in teams_with_multiple:
            team_data = teams_sorted[teams_sorted["team_id"] == team_id]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∏–ª—è
            for i in range(1, len(team_data)):
                prev_season = team_data.iloc[i - 1]
                curr_season = team_data.iloc[i]

                if prev_season["playing_style"] != curr_season["playing_style"]:
                    style_changes.append({
                        "team_id": team_id,
                        "team_name": team_data.iloc[0]["team_name"],
                        "league_name": team_data.iloc[0]["league_name"],
                        "from_season": prev_season["season_code"],
                        "to_season": curr_season["season_code"],
                        "from_style": prev_season["playing_style"],
                        "to_style": curr_season["playing_style"],
                        "change_description": f"{prev_season['playing_style']} ‚Üí {curr_season['playing_style']}"
                    })

        if style_changes:
            changes_df = pd.DataFrame(style_changes)
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(changes_df)} –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∏–ª–µ–π")

            # –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
            common_changes = changes_df["change_description"].value_counts().head(5)
            logger.info("–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∏–ª–µ–π:")
            for change, count in common_changes.items():
                logger.info(f"  ‚Ä¢ {change}: {count} –∫–æ–º–∞–Ω–¥")

            return changes_df
        else:
            logger.info("–ò–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∏–ª–µ–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            return pd.DataFrame()

    def _save_style_analysis_results(self, analysis_results: Dict):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∏–ª–µ–π
        Args:
            analysis_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        import os
        from datetime import datetime

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = f"data/team_styles_analysis_{timestamp}"
        os.makedirs(results_dir, exist_ok=True)

        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {results_dir}")

        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—ã —Å–æ —Å—Ç–∏–ª—è–º–∏
        teams_path = os.path.join(results_dir, "teams_with_playing_styles.csv")
        analysis_results["teams_with_styles"].to_csv(teams_path, index=False, encoding='utf-8-sig')
        logger.info(f"‚úÖ –ö–æ–º–∞–Ω–¥—ã —Å–æ —Å—Ç–∏–ª—è–º–∏: {teams_path}")

        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        clusters_path = os.path.join(results_dir, "cluster_analysis.csv")
        analysis_results["cluster_analysis"].to_csv(clusters_path, index=False, encoding='utf-8-sig')
        logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {clusters_path}")

        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ª–∏–≥–∞–º
        leagues_path = os.path.join(results_dir, "league_distribution.csv")
        analysis_results["league_distribution"].to_csv(leagues_path, encoding='utf-8-sig')
        logger.info(f"‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ª–∏–≥–∞–º: {leagues_path}")

        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∏–ª–µ–π (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if len(analysis_results["style_changes"]) > 0:
            changes_path = os.path.join(results_dir, "style_changes.csv")
            analysis_results["style_changes"].to_csv(changes_path, index=False, encoding='utf-8-sig')
            logger.info(f"‚úÖ –ò–∑–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∏–ª–µ–π: {changes_path}")

        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        report_path = os.path.join(results_dir, "analysis_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("–ê–ù–ê–õ–ò–ó –ò–ì–†–û–í–´–• –°–¢–ò–õ–ï–ô –ö–û–ú–ê–ù–î\n")
            f.write("=" * 70 + "\n\n")

            f.write(f"–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–í—Å–µ–≥–æ –∫–æ–º–∞–Ω–¥ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(analysis_results['teams_with_styles'])}\n")
            f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∏–ª–µ–π: {analysis_results['n_clusters']}\n")
            f.write(f"–ö–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—Å–∏–ª—É—ç—Ç): {analysis_results['silhouette_score']:.3f}\n\n")

            f.write("–°–¢–ò–õ–ò –ò–ì–†–´:\n")
            f.write("-" * 50 + "\n")
            for _, row in analysis_results["cluster_analysis"].iterrows():
                f.write(f"\n{row['style_name']} (–ö–ª–∞—Å—Ç–µ—Ä {row['cluster_id']}):\n")
                f.write(f"  ‚Ä¢ –ö–æ–º–∞–Ω–¥: {row['team_count']} ({row['percentage']:.1f}%)\n")
                f.write(f"  ‚Ä¢ –ê—Ç–∞–∫–∞: {row['avg_attacking']:.2f} –≥–æ–ª/90 –º–∏–Ω\n")
                f.write(f"  ‚Ä¢ –í–ª–∞–¥–µ–Ω–∏–µ: {row['avg_possession']:.1f}%\n")
                f.write(f"  ‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {row['avg_efficiency']:.2f}\n")
                f.write(f"  ‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥: {', '.join(row['top_teams'][:3])}\n")

        logger.info(f"‚úÖ –û—Ç—á–µ—Ç: {report_path}")
        logger.info(f"üìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: {os.path.abspath(results_dir)}")

    def get_team_style_recommendations(
            self,
            team_name: str,
            season: Optional[str] = None
    ) -> Dict:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏–≥—Ä–æ–≤–æ–º—É —Å—Ç–∏–ª—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–º–∞–Ω–¥—ã
        Args:
            team_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
            season: –°–µ–∑–æ–Ω (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω - –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π)
        Returns:
            Dict: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑ –∫–æ–º–∞–Ω–¥—ã
        """
        self.initialize_spark()

        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –∫–æ–º–∞–Ω–¥—ã: {team_name}")

        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–º–∞–Ω–¥—É –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∞–Ω–∞–ª–∏–∑–∞
        if hasattr(self, 'last_style_analysis'):
            teams_df = self.last_style_analysis['teams_with_styles']

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∫–æ–º–∞–Ω–¥—ã
            team_data = teams_df[teams_df['team_name'] == team_name]

            if not team_data.empty:
                if season:
                    team_data = team_data[team_data['season_code'] == season]

                if not team_data.empty:
                    team_row = team_data.iloc[0]

                    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                    recommendations = self._generate_team_recommendations(team_row)

                    return {
                        'team_name': team_name,
                        'season': team_row['season_code'],
                        'league': team_row['league_name'],
                        'current_style': team_row['playing_style'],
                        'cluster': team_row['cluster'],
                        'attacking_power': float(team_row['attacking_power']),
                        'possession_control': float(team_row['possession_control']),
                        'attack_efficiency': float(team_row['attack_efficiency']),
                        'recommendations': recommendations
                    }

        logger.warning(f"–ö–æ–º–∞–Ω–¥–∞ {team_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∞–Ω–∞–ª–∏–∑–∞")
        return {}

    def _generate_team_recommendations(self, team_row: pd.Series) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—ë –º–µ—Ç—Ä–∏–∫
        Args:
            team_row: –î–∞–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        recommendations = []

        attacking = team_row['attacking_power']
        possession = team_row['possession_control']
        efficiency = team_row['attack_efficiency']
        creativity = team_row.get('creativity', 0)
        aggressiveness = team_row.get('aggressiveness', 0)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—Ç–∞–∫—É—é—â–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª
        if attacking < 1.0:
            recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å –∞—Ç–∞–∫—É—é—â–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª: –±–æ–ª—å—à–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –æ–ø–∞—Å–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤")
        elif attacking > 1.8:
            recommendations.append("–£–ª—É—á—à–∏—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é: –±–æ–ª—å—à–µ –≥–æ–ª–æ–≤ –∏–∑ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–ª–∞–¥–µ–Ω–∏–µ –º—è—á–æ–º
        if possession < 45:
            recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å –≤–ª–∞–¥–µ–Ω–∏–µ –º—è—á–æ–º: –±–æ–ª—å—à–µ –∫–æ–Ω—Ç—Ä–æ–ª—è –∏ —Ç–æ—á–Ω—ã—Ö –ø–µ—Ä–µ–¥–∞—á")
        elif possession > 60:
            recommendations.append("–°–¥–µ–ª–∞—Ç—å –≤–ª–∞–¥–µ–Ω–∏–µ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º: –±–æ–ª—å—à–µ –æ–ø–∞—Å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –ø—Ä–∏ –≤–ª–∞–¥–µ–Ω–∏–∏")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        if efficiency < 0.9:
            recommendations.append("–ü–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞—Ç–∞–∫–∏: —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –º–æ–º–µ–Ω—Ç–æ–≤")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        if creativity < 0.5:
            recommendations.append("–†–∞–∑–≤–∏–≤–∞—Ç—å –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å: –±–æ–ª—å—à–µ –≥–æ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–¥–∞—á –∏ –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Å–æ–≤")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å
        if aggressiveness > 80:
            recommendations.append("–°–Ω–∏–∑–∏—Ç—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å: –º–µ–Ω—å—à–µ —Ñ–æ–ª–æ–≤ –∏ –∂–µ–ª—Ç—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫")
        elif aggressiveness < 40:
            recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å –≤ –æ–±–æ—Ä–æ–Ω–µ: –±–æ–ª—å—à–µ –ø—Ä–µ—Å—Å–∏–Ω–≥–∞")

        return recommendations


    
    def calculate_home_away_win_rate(
        self, 
        league_filter: Optional[str] = None,
        season_filter: Optional[str] = None,
        top_n: int = 10
    ) -> pd.DataFrame:
        """
        –ó–ê–î–ê–ß–ê 2: –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–±–µ–¥ (–¥–æ–º–∞/–≤ –≥–æ—Å—Ç—è—Ö)
        
        –í—ã–ø–æ–ª–Ω—è–µ—Ç Spark SQL –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –ø–æ–±–µ–¥ –∫–æ–º–∞–Ω–¥
        –¥–æ–º–∞ –∏ –≤ –≥–æ—Å—Ç—è—Ö.
        
        Args:
            league_filter: –§–∏–ª—å—Ç—Ä –ø–æ –ª–∏–≥–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'epl')
            season_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∑–æ–Ω—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, '2024-2025')
            top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø –∫–æ–º–∞–Ω–¥ –¥–ª—è –≤—ã–≤–æ–¥–∞
        
        Returns:
            pd.DataFrame: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (pandas –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏)
        """
        self.initialize_spark()
        
        logger.info("=" * 70)
        logger.info("–ó–ê–î–ê–ß–ê 2: –†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –ø–æ–±–µ–¥ –¥–æ–º–∞/–≤ –≥–æ—Å—Ç—è—Ö —á–µ—Ä–µ–∑ Spark SQL")
        logger.info("=" * 70)
        
        # –ß–∏—Ç–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ PostgreSQL
        matches_df = self.read_table_from_postgres("matches")
        teams_df = self.read_table_from_postgres("teams")
        leagues_df = self.read_table_from_postgres("leagues")
        seasons_df = self.read_table_from_postgres("seasons")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è Spark SQL
        matches_df.createOrReplaceTempView("matches")
        teams_df.createOrReplaceTempView("teams")
        leagues_df.createOrReplaceTempView("leagues")
        seasons_df.createOrReplaceTempView("seasons")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º WHERE —É—Å–ª–æ–≤–∏—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        where_conditions = ["m.home_goals IS NOT NULL", "m.away_goals IS NOT NULL"]
        
        if league_filter:
            where_conditions.append(f"l.league_code = '{league_filter}'")
        
        if season_filter:
            where_conditions.append(f"s.season_code = '{season_filter}'")
        
        where_clause = " AND ".join(where_conditions)
        
        # Spark SQL –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –ø–æ–±–µ–¥
        spark_sql_query = f"""
        WITH team_matches AS (
            -- –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –º–∞—Ç—á–∏ –∫–æ–º–∞–Ω–¥—ã (–¥–æ–º–∞ –∏ –≤ –≥–æ—Å—Ç—è—Ö)
            SELECT 
                t.team_id,
                t.team_name,
                l.league_name,
                m.match_id,
                m.home_team_id,
                m.away_team_id,
                m.home_goals,
                m.away_goals
            FROM teams t
            JOIN matches m ON (t.team_id = m.home_team_id OR t.team_id = m.away_team_id)
            JOIN leagues l ON m.league_id = l.league_id
            JOIN seasons s ON m.season_id = s.season_id
            WHERE {where_clause}
        ),
        home_stats AS (
            -- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–º–∞—à–Ω–∏—Ö –º–∞—Ç—á–µ–π
            SELECT 
                team_id,
                team_name,
                league_name,
                COUNT(*) as home_matches,
                SUM(CASE WHEN home_team_id = team_id AND home_goals > away_goals THEN 1 ELSE 0 END) as home_wins
            FROM team_matches
            WHERE home_team_id = team_id
            GROUP BY team_id, team_name, league_name
        ),
        away_stats AS (
            -- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–µ–∑–¥–Ω—ã—Ö –º–∞—Ç—á–µ–π
            SELECT 
                team_id,
                COUNT(*) as away_matches,
                SUM(CASE WHEN away_team_id = team_id AND away_goals > home_goals THEN 1 ELSE 0 END) as away_wins
            FROM team_matches
            WHERE away_team_id = team_id
            GROUP BY team_id
        )
        -- –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å —Ä–∞—Å—á–µ—Ç–æ–º –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        SELECT 
            h.team_id,
            h.team_name,
            h.league_name,
            h.home_matches,
            h.home_wins,
            ROUND((h.home_wins * 100.0) / NULLIF(h.home_matches, 0), 2) as home_win_pct,
            a.away_matches,
            a.away_wins,
            ROUND((a.away_wins * 100.0) / NULLIF(a.away_matches, 0), 2) as away_win_pct,
            ROUND(
                ((h.home_wins + a.away_wins) * 100.0) / NULLIF((h.home_matches + a.away_matches), 0), 
                2
            ) as total_win_pct
        FROM home_stats h
        JOIN away_stats a ON h.team_id = a.team_id
        ORDER BY total_win_pct DESC, home_win_pct DESC
        LIMIT {top_n}
        """
        
        logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Spark SQL –∑–∞–ø—Ä–æ—Å–∞...")
        logger.info(f"–§–∏–ª—å—Ç—Ä—ã: –ª–∏–≥–∞={league_filter or '–≤—Å–µ'}, —Å–µ–∑–æ–Ω={season_filter or '–≤—Å–µ'}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        result_df = self.spark.sql(spark_sql_query)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–æ–Ω—Å–æ–ª–∏
        logger.info(f"\nüèÜ –¢–æ–ø-{top_n} –∫–æ–º–∞–Ω–¥ –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É –ø–æ–±–µ–¥:")
        result_df.show(truncate=False)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ pandas –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        pandas_df = result_df.toPandas()
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ù–∞–π–¥–µ–Ω–æ –∫–æ–º–∞–Ω–¥: {len(pandas_df)}")
        
        return pandas_df
    
    def calculate_team_dynamics(
        self,
        league_filter: Optional[str] = None,
        season_filter: Optional[str] = None,
        team_names: Optional[List[str]] = None,
        output_parquet_path: Optional[str] = None
    ) -> pd.DataFrame:
        """
        –ó–ê–î–ê–ß–ê 3: –î–∏–Ω–∞–º–∏–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å–µ–∑–æ–Ω–∞–º/–º–µ—Å—è—Ü–∞–º
        
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–∞–Ω–¥—ã –ø–æ —Ö–æ–¥—É —Å–µ–∑–æ–Ω–∞
        —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–∫–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π Spark.
        
        Args:
            league_filter: –§–∏–ª—å—Ç—Ä –ø–æ –∫–æ–¥—É –ª–∏–≥–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'epl')
            season_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–µ–∑–æ–Ω—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, '2023-2024')
            team_names: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–∞–Ω–¥ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None ‚Äî –≤—Å–µ –∫–æ–º–∞–Ω–¥—ã)
            output_parquet_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ Parquet
        
        Returns:
            pd.DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:
                - team_id, team_name, league_name, season_code
                - match_date, match_number
                - points, goal_diff (–∑–∞ –º–∞—Ç—á)
                - cumulative_points, cumulative_goal_diff (–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ)
                - goals_for, goals_against (–∑–∞ –º–∞—Ç—á)
        """
        self.initialize_spark()
        
        logger.info("=" * 70)
        logger.info("–ó–ê–î–ê–ß–ê 3: –î–∏–Ω–∞–º–∏–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å–µ–∑–æ–Ω–∞–º (Spark Window Functions)")
        logger.info("=" * 70)
        
        # –ß–∏—Ç–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ PostgreSQL
        matches_df = self.read_table_from_postgres("matches")
        teams_df = self.read_table_from_postgres("teams")
        leagues_df = self.read_table_from_postgres("leagues")
        seasons_df = self.read_table_from_postgres("seasons")
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è Spark SQL
        matches_df.createOrReplaceTempView("matches")
        teams_df.createOrReplaceTempView("teams")
        leagues_df.createOrReplaceTempView("leagues")
        seasons_df.createOrReplaceTempView("seasons")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º WHERE —É—Å–ª–æ–≤–∏—è
        where_conditions = ["m.home_goals IS NOT NULL", "m.away_goals IS NOT NULL"]
        
        if league_filter:
            where_conditions.append(f"l.league_code = '{league_filter}'")
        
        if season_filter:
            where_conditions.append(f"s.season_code = '{season_filter}'")
        
        where_clause = " AND ".join(where_conditions)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–æ–º–∞–Ω–¥–∞–º (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        team_filter_clause = ""
        if team_names:
            team_names_str = ", ".join([f"'{name}'" for name in team_names])
            team_filter_clause = f"AND t.team_name IN ({team_names_str})"
        
        logger.info(f"–§–∏–ª—å—Ç—Ä—ã: –ª–∏–≥–∞={league_filter or '–≤—Å–µ'}, —Å–µ–∑–æ–Ω={season_filter or '–≤—Å–µ'}")
        if team_names:
            logger.info(f"–ö–æ–º–∞–Ω–¥—ã: {', '.join(team_names)}")
        
        # Spark SQL –∑–∞–ø—Ä–æ—Å: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –¥–æ–º–∞—à–Ω–∏–µ –∏ –≥–æ—Å—Ç–µ–≤—ã–µ –º–∞—Ç—á–∏
        # –∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–∞–Ω–¥—ã
        spark_sql_query = f"""
        WITH all_team_matches AS (
            -- –î–æ–º–∞—à–Ω–∏–µ –º–∞—Ç—á–∏
            SELECT 
                t.team_id,
                t.team_name,
                l.league_id,
                l.league_name,
                l.league_code,
                s.season_id,
                s.season_code,
                m.match_id,
                m.match_date,
                m.home_goals as goals_for,
                m.away_goals as goals_against,
                CASE 
                    WHEN m.home_goals > m.away_goals THEN 3
                    WHEN m.home_goals = m.away_goals THEN 1
                    ELSE 0
                END as points,
                (m.home_goals - m.away_goals) as goal_diff,
                'home' as venue_type
            FROM matches m
            JOIN teams t ON m.home_team_id = t.team_id
            JOIN leagues l ON m.league_id = l.league_id
            JOIN seasons s ON m.season_id = s.season_id
            WHERE {where_clause} {team_filter_clause}
            
            UNION ALL
            
            -- –ì–æ—Å—Ç–µ–≤—ã–µ –º–∞—Ç—á–∏
            SELECT 
                t.team_id,
                t.team_name,
                l.league_id,
                l.league_name,
                l.league_code,
                s.season_id,
                s.season_code,
                m.match_id,
                m.match_date,
                m.away_goals as goals_for,
                m.home_goals as goals_against,
                CASE 
                    WHEN m.away_goals > m.home_goals THEN 3
                    WHEN m.away_goals = m.home_goals THEN 1
                    ELSE 0
                END as points,
                (m.away_goals - m.home_goals) as goal_diff,
                'away' as venue_type
            FROM matches m
            JOIN teams t ON m.away_team_id = t.team_id
            JOIN leagues l ON m.league_id = l.league_id
            JOIN seasons s ON m.season_id = s.season_id
            WHERE {where_clause} {team_filter_clause}
        )
        SELECT 
            team_id,
            team_name,
            league_id,
            league_name,
            league_code,
            season_id,
            season_code,
            match_id,
            match_date,
            goals_for,
            goals_against,
            points,
            goal_diff,
            venue_type
        FROM all_team_matches
        ORDER BY team_id, season_id, match_date
        """
        
        logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ Spark SQL –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–∞—Ç—á–µ–π...")
        base_df = self.spark.sql(spark_sql_query)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–∫–æ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        logger.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–∫–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫–Ω–æ: –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∫–æ–º–∞–Ω–¥–µ –∏ —Å–µ–∑–æ–Ω—É, —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
        window_spec = Window.partitionBy("team_id", "season_id").orderBy("match_date")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        result_df = base_df \
            .withColumn("match_number", F.row_number().over(window_spec)) \
            .withColumn("cumulative_points", F.sum("points").over(window_spec)) \
            .withColumn("cumulative_goal_diff", F.sum("goal_diff").over(window_spec)) \
            .withColumn("cumulative_goals_for", F.sum("goals_for").over(window_spec)) \
            .withColumn("cumulative_goals_against", F.sum("goals_against").over(window_spec))
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–æ–Ω—Å–æ–ª–∏
        logger.info(f"\nüìä –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–∏–Ω–∞–º–∏–∫–∏ –∫–æ–º–∞–Ω–¥:")
        result_df.show(20, truncate=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
        if output_parquet_path:
            parquet_path = Path(output_parquet_path)
            parquet_path.parent.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Parquet: {output_parquet_path}")
            result_df.write.mode("overwrite").parquet(str(parquet_path))
            logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Parquet: {output_parquet_path}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ pandas –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        pandas_df = result_df.toPandas()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        unique_teams = pandas_df['team_name'].nunique()
        unique_seasons = pandas_df['season_code'].nunique()
        total_records = len(pandas_df)
        
        logger.info(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        logger.info(f"   - –ö–æ–º–∞–Ω–¥: {unique_teams}")
        logger.info(f"   - –°–µ–∑–æ–Ω–æ–≤: {unique_seasons}")
        logger.info(f"   - –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_records}")
        
        return pandas_df
    
    def load_dynamics_from_parquet(self, parquet_path: str) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–∏–Ω–∞–º–∏–∫–∏ –∏–∑ Parquet —Ñ–∞–π–ª–∞
        
        Args:
            parquet_path: –ü—É—Ç—å –∫ Parquet —Ñ–∞–π–ª—É
        
        Returns:
            pd.DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–∏–Ω–∞–º–∏–∫–∏
        """
        self.initialize_spark()
        
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Parquet: {parquet_path}")
        
        df = self.spark.read.parquet(parquet_path)
        pandas_df = df.toPandas()
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(pandas_df)} –∑–∞–ø–∏—Å–µ–π")
        
        return pandas_df
    
    def get_detailed_match_statistics(self) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –º–∞—Ç—á–∞–º
        
        Returns:
            pd.DataFrame: DataFrame —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        self.initialize_spark()
        
        query = """
        SELECT 
            m.match_id,
            m.match_date,
            l.league_name,
            s.season_code,
            ht.team_name as home_team,
            at.team_name as away_team,
            m.home_goals,
            m.away_goals,
            m.home_xg,
            m.away_xg,
            CASE 
                WHEN m.home_goals > m.away_goals THEN 'HOME_WIN'
                WHEN m.home_goals < m.away_goals THEN 'AWAY_WIN'
                ELSE 'DRAW'
            END as result
        FROM matches m
        JOIN teams ht ON m.home_team_id = ht.team_id
        JOIN teams at ON m.away_team_id = at.team_id
        JOIN leagues l ON m.league_id = l.league_id
        JOIN seasons s ON m.season_id = s.season_id
        WHERE m.home_goals IS NOT NULL
        ORDER BY m.match_date DESC
        """
        
        result_df = self.read_query_from_postgres(query)
        return result_df.toPandas()
    
    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç Spark —Å–µ—Å—Å–∏—é"""
        if self.spark is not None:
            logger.info("–ó–∞–∫—Ä—ã—Ç–∏–µ Spark —Å–µ—Å—Å–∏–∏...")
            self.spark_config.stop_spark_session()
            self.spark = None
            logger.info("‚úÖ Spark —Å–µ—Å—Å–∏—è –∑–∞–∫—Ä—ã—Ç–∞")
    
    def __enter__(self):
        """Context manager: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Spark –ø—Ä–∏ –≤—Ö–æ–¥–µ"""
        self.initialize_spark()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager: –∑–∞–∫—Ä—ã–≤–∞–µ—Ç Spark –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ"""
        self.close()

