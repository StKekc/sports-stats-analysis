"""
ÐœÐ¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ñ‡ÐµÑ€ÐµÐ· Apache Spark SQL
"""

import logging
from typing import Dict, Optional
import pandas as pd
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F
from .spark_config import SparkConfig


logger = logging.getLogger(__name__)


class SparkProcessor:
    """
    ÐšÐ»Ð°ÑÑ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Apache Spark
    Ð§Ð¸Ñ‚Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· PostgreSQL, Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· Spark SQL
    """
    
    def __init__(self, db_config: Dict[str, str], spark_config: Optional[SparkConfig] = None):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°
        
        Args:
            db_config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ðº PostgreSQL
            spark_config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Spark (ÐµÑÐ»Ð¸ None, ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ÑÑ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ)
        """
        self.db_config = db_config
        self.spark_config = spark_config or SparkConfig()
        self.spark: Optional[SparkSession] = None
        self.jdbc_url = self.spark_config.get_postgres_jdbc_url(db_config)
        self.jdbc_properties = self.spark_config.get_jdbc_properties(db_config)
    
    def initialize_spark(self):
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Spark ÑÐµÑÑÐ¸ÑŽ"""
        if self.spark is None:
            logger.info("Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Spark ÑÐµÑÑÐ¸Ð¸...")
            self.spark = self.spark_config.create_spark_session()
            logger.info("âœ… Spark ÑÐµÑÑÐ¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð°")
    
    def read_table_from_postgres(self, table_name: str) -> DataFrame:
        """
        Ð§Ð¸Ñ‚Ð°ÐµÑ‚ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ð¸Ð· PostgreSQL Ð² Spark DataFrame
        
        Args:
            table_name: ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹
        
        Returns:
            DataFrame: Spark DataFrame Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹
        """
        self.initialize_spark()
        
        logger.info(f"Ð§Ñ‚ÐµÐ½Ð¸Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ '{table_name}' Ð¸Ð· PostgreSQL...")
        
        df = self.spark.read.jdbc(
            url=self.jdbc_url,
            table=table_name,
            properties=self.jdbc_properties
        )
        
        count = df.count()
        logger.info(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {count} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¸Ð· Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ '{table_name}'")
        
        return df
    
    def read_query_from_postgres(self, query: str) -> DataFrame:
        """
        Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº PostgreSQL Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÐºÐ°Ðº Spark DataFrame
        
        Args:
            query: SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ
        
        Returns:
            DataFrame: Spark DataFrame Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
        """
        self.initialize_spark()
        
        logger.info(f"Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ðº PostgreSQL...")
        
        df = self.spark.read.jdbc(
            url=self.jdbc_url,
            table=f"({query}) as query",
            properties=self.jdbc_properties
        )
        
        return df
    
    def calculate_home_away_win_rate(
        self, 
        league_filter: Optional[str] = None,
        season_filter: Optional[str] = None,
        top_n: int = 10
    ) -> pd.DataFrame:
        """
        Ð—ÐÐ”ÐÐ§Ð 2: ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð¿Ð¾Ð±ÐµÐ´ (Ð´Ð¾Ð¼Ð°/Ð² Ð³Ð¾ÑÑ‚ÑÑ…)
        
        Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Spark SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð° Ð¿Ð¾Ð±ÐµÐ´ ÐºÐ¾Ð¼Ð°Ð½Ð´
        Ð´Ð¾Ð¼Ð° Ð¸ Ð² Ð³Ð¾ÑÑ‚ÑÑ….
        
        Args:
            league_filter: Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ð»Ð¸Ð³Ðµ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 'epl')
            season_filter: Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ ÑÐµÐ·Ð¾Ð½Ñƒ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, '2024-2025')
            top_n: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾Ð¿ ÐºÐ¾Ð¼Ð°Ð½Ð´ Ð´Ð»Ñ Ð²Ñ‹Ð²Ð¾Ð´Ð°
        
        Returns:
            pd.DataFrame: DataFrame Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ (pandas Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸)
        """
        self.initialize_spark()
        
        logger.info("=" * 70)
        logger.info("Ð—ÐÐ”ÐÐ§Ð 2: Ð Ð°ÑÑ‡ÐµÑ‚ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð° Ð¿Ð¾Ð±ÐµÐ´ Ð´Ð¾Ð¼Ð°/Ð² Ð³Ð¾ÑÑ‚ÑÑ… Ñ‡ÐµÑ€ÐµÐ· Spark SQL")
        logger.info("=" * 70)
        
        # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¸Ð· PostgreSQL
        matches_df = self.read_table_from_postgres("matches")
        teams_df = self.read_table_from_postgres("teams")
        leagues_df = self.read_table_from_postgres("leagues")
        seasons_df = self.read_table_from_postgres("seasons")
        
        # Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð´Ð»Ñ Spark SQL
        matches_df.createOrReplaceTempView("matches")
        teams_df.createOrReplaceTempView("teams")
        leagues_df.createOrReplaceTempView("leagues")
        seasons_df.createOrReplaceTempView("seasons")
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ WHERE ÑƒÑÐ»Ð¾Ð²Ð¸Ñ Ð´Ð»Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸
        where_conditions = ["m.home_goals IS NOT NULL", "m.away_goals IS NOT NULL"]
        
        if league_filter:
            where_conditions.append(f"l.league_code = '{league_filter}'")
        
        if season_filter:
            where_conditions.append(f"s.season_code = '{season_filter}'")
        
        where_clause = " AND ".join(where_conditions)
        
        # Spark SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð° Ð¿Ð¾Ð±ÐµÐ´
        spark_sql_query = f"""
        WITH team_matches AS (
            -- ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð¼Ð°Ñ‚Ñ‡Ð¸ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ (Ð´Ð¾Ð¼Ð° Ð¸ Ð² Ð³Ð¾ÑÑ‚ÑÑ…)
            SELECT 
                t.team_id,
                t.team_name,
                l.league_name,
                m.match_id,
                m.home_team_id,
                m.away_team_id,
                m.home_goals,
                m.away_goals
            FROM teams t
            JOIN matches m ON (t.team_id = m.home_team_id OR t.team_id = m.away_team_id)
            JOIN leagues l ON m.league_id = l.league_id
            JOIN seasons s ON m.season_id = s.season_id
            WHERE {where_clause}
        ),
        home_stats AS (
            -- Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð´Ð¾Ð¼Ð°ÑˆÐ½Ð¸Ñ… Ð¼Ð°Ñ‚Ñ‡ÐµÐ¹
            SELECT 
                team_id,
                team_name,
                league_name,
                COUNT(*) as home_matches,
                SUM(CASE WHEN home_team_id = team_id AND home_goals > away_goals THEN 1 ELSE 0 END) as home_wins
            FROM team_matches
            WHERE home_team_id = team_id
            GROUP BY team_id, team_name, league_name
        ),
        away_stats AS (
            -- Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð²Ñ‹ÐµÐ·Ð´Ð½Ñ‹Ñ… Ð¼Ð°Ñ‚Ñ‡ÐµÐ¹
            SELECT 
                team_id,
                COUNT(*) as away_matches,
                SUM(CASE WHEN away_team_id = team_id AND away_goals > home_goals THEN 1 ELSE 0 END) as away_wins
            FROM team_matches
            WHERE away_team_id = team_id
            GROUP BY team_id
        )
        -- Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð¾Ð¼ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð¾Ð²
        SELECT 
            h.team_id,
            h.team_name,
            h.league_name,
            h.home_matches,
            h.home_wins,
            ROUND((h.home_wins * 100.0) / NULLIF(h.home_matches, 0), 2) as home_win_pct,
            a.away_matches,
            a.away_wins,
            ROUND((a.away_wins * 100.0) / NULLIF(a.away_matches, 0), 2) as away_win_pct,
            ROUND(
                ((h.home_wins + a.away_wins) * 100.0) / NULLIF((h.home_matches + a.away_matches), 0), 
                2
            ) as total_win_pct
        FROM home_stats h
        JOIN away_stats a ON h.team_id = a.team_id
        ORDER BY total_win_pct DESC, home_win_pct DESC
        LIMIT {top_n}
        """
        
        logger.info("Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Spark SQL Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°...")
        logger.info(f"Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹: Ð»Ð¸Ð³Ð°={league_filter or 'Ð²ÑÐµ'}, ÑÐµÐ·Ð¾Ð½={season_filter or 'Ð²ÑÐµ'}")
        
        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ
        result_df = self.spark.sql(spark_sql_query)
        
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð² ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸
        logger.info(f"\nðŸ† Ð¢Ð¾Ð¿-{top_n} ÐºÐ¾Ð¼Ð°Ð½Ð´ Ð¿Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ñƒ Ð¿Ð¾Ð±ÐµÐ´:")
        result_df.show(truncate=False)
        
        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² pandas Ð´Ð»Ñ Ð´Ð°Ð»ÑŒÐ½ÐµÐ¹ÑˆÐµÐ¹ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        pandas_df = result_df.toPandas()
        
        logger.info(f"âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°. ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´: {len(pandas_df)}")
        
        return pandas_df
    
    def get_detailed_match_statistics(self) -> pd.DataFrame:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½ÑƒÑŽ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¿Ð¾ Ð²ÑÐµÐ¼ Ð¼Ð°Ñ‚Ñ‡Ð°Ð¼
        
        Returns:
            pd.DataFrame: DataFrame Ñ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹
        """
        self.initialize_spark()
        
        query = """
        SELECT 
            m.match_id,
            m.match_date,
            l.league_name,
            s.season_code,
            ht.team_name as home_team,
            at.team_name as away_team,
            m.home_goals,
            m.away_goals,
            m.home_xg,
            m.away_xg,
            CASE 
                WHEN m.home_goals > m.away_goals THEN 'HOME_WIN'
                WHEN m.home_goals < m.away_goals THEN 'AWAY_WIN'
                ELSE 'DRAW'
            END as result
        FROM matches m
        JOIN teams ht ON m.home_team_id = ht.team_id
        JOIN teams at ON m.away_team_id = at.team_id
        JOIN leagues l ON m.league_id = l.league_id
        JOIN seasons s ON m.season_id = s.season_id
        WHERE m.home_goals IS NOT NULL
        ORDER BY m.match_date DESC
        """
        
        result_df = self.read_query_from_postgres(query)
        return result_df.toPandas()
    
    def close(self):
        """Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÑ‚ Spark ÑÐµÑÑÐ¸ÑŽ"""
        if self.spark is not None:
            logger.info("Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Spark ÑÐµÑÑÐ¸Ð¸...")
            self.spark_config.stop_spark_session()
            self.spark = None
            logger.info("âœ… Spark ÑÐµÑÑÐ¸Ñ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ð°")
    
    def __enter__(self):
        """Context manager: Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Spark Ð¿Ñ€Ð¸ Ð²Ñ…Ð¾Ð´Ðµ"""
        self.initialize_spark()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager: Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÑ‚ Spark Ð¿Ñ€Ð¸ Ð²Ñ‹Ñ…Ð¾Ð´Ðµ"""
        self.close()

