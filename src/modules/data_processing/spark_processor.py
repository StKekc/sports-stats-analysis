"""
ÐœÐ¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ñ‡ÐµÑ€ÐµÐ· Apache Spark SQL
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional
import pandas as pd
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from .spark_config import SparkConfig


logger = logging.getLogger(__name__)


class SparkProcessor:
    """
    ÐšÐ»Ð°ÑÑ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Apache Spark
    Ð§Ð¸Ñ‚Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· PostgreSQL, Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ†Ð¸Ð¸ Ñ‡ÐµÑ€ÐµÐ· Spark SQL
    """
    
    def __init__(self, db_config: Dict[str, str], spark_config: Optional[SparkConfig] = None):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€Ð°
        
        Args:
            db_config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ðº PostgreSQL
            spark_config: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Spark (ÐµÑÐ»Ð¸ None, ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ÑÑ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ)
        """
        self.db_config = db_config
        self.spark_config = spark_config or SparkConfig()
        self.spark: Optional[SparkSession] = None
        self.jdbc_url = self.spark_config.get_postgres_jdbc_url(db_config)
        self.jdbc_properties = self.spark_config.get_jdbc_properties(db_config)
    
    def initialize_spark(self):
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Spark ÑÐµÑÑÐ¸ÑŽ"""
        if self.spark is None:
            logger.info("Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Spark ÑÐµÑÑÐ¸Ð¸...")
            self.spark = self.spark_config.create_spark_session()
            logger.info("âœ… Spark ÑÐµÑÑÐ¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð°")
    
    def read_table_from_postgres(self, table_name: str) -> DataFrame:
        """
        Ð§Ð¸Ñ‚Ð°ÐµÑ‚ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ð¸Ð· PostgreSQL Ð² Spark DataFrame
        
        Args:
            table_name: ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹
        
        Returns:
            DataFrame: Spark DataFrame Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹
        """
        self.initialize_spark()
        
        logger.info(f"Ð§Ñ‚ÐµÐ½Ð¸Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ '{table_name}' Ð¸Ð· PostgreSQL...")
        
        df = self.spark.read.jdbc(
            url=self.jdbc_url,
            table=table_name,
            properties=self.jdbc_properties
        )
        
        count = df.count()
        logger.info(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {count} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¸Ð· Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ '{table_name}'")
        
        return df
    
    def read_query_from_postgres(self, query: str) -> DataFrame:
        """
        Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº PostgreSQL Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÐºÐ°Ðº Spark DataFrame
        
        Args:
            query: SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ
        
        Returns:
            DataFrame: Spark DataFrame Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
        """
        self.initialize_spark()
        
        logger.info(f"Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ðº PostgreSQL...")
        
        df = self.spark.read.jdbc(
            url=self.jdbc_url,
            table=f"({query}) as query",
            properties=self.jdbc_properties
        )
        
        return df
    
    def calculate_home_away_win_rate(
        self, 
        league_filter: Optional[str] = None,
        season_filter: Optional[str] = None,
        top_n: int = 10
    ) -> pd.DataFrame:
        """
        Ð—ÐÐ”ÐÐ§Ð 2: ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð¿Ð¾Ð±ÐµÐ´ (Ð´Ð¾Ð¼Ð°/Ð² Ð³Ð¾ÑÑ‚ÑÑ…)
        
        Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Spark SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð° Ð¿Ð¾Ð±ÐµÐ´ ÐºÐ¾Ð¼Ð°Ð½Ð´
        Ð´Ð¾Ð¼Ð° Ð¸ Ð² Ð³Ð¾ÑÑ‚ÑÑ….
        
        Args:
            league_filter: Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ð»Ð¸Ð³Ðµ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 'epl')
            season_filter: Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ ÑÐµÐ·Ð¾Ð½Ñƒ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, '2024-2025')
            top_n: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾Ð¿ ÐºÐ¾Ð¼Ð°Ð½Ð´ Ð´Ð»Ñ Ð²Ñ‹Ð²Ð¾Ð´Ð°
        
        Returns:
            pd.DataFrame: DataFrame Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ (pandas Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸)
        """
        self.initialize_spark()
        
        logger.info("=" * 70)
        logger.info("Ð—ÐÐ”ÐÐ§Ð 2: Ð Ð°ÑÑ‡ÐµÑ‚ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð° Ð¿Ð¾Ð±ÐµÐ´ Ð´Ð¾Ð¼Ð°/Ð² Ð³Ð¾ÑÑ‚ÑÑ… Ñ‡ÐµÑ€ÐµÐ· Spark SQL")
        logger.info("=" * 70)
        
        # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¸Ð· PostgreSQL
        matches_df = self.read_table_from_postgres("matches")
        teams_df = self.read_table_from_postgres("teams")
        leagues_df = self.read_table_from_postgres("leagues")
        seasons_df = self.read_table_from_postgres("seasons")
        
        # Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð´Ð»Ñ Spark SQL
        matches_df.createOrReplaceTempView("matches")
        teams_df.createOrReplaceTempView("teams")
        leagues_df.createOrReplaceTempView("leagues")
        seasons_df.createOrReplaceTempView("seasons")
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ WHERE ÑƒÑÐ»Ð¾Ð²Ð¸Ñ Ð´Ð»Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸
        where_conditions = ["m.home_goals IS NOT NULL", "m.away_goals IS NOT NULL"]
        
        if league_filter:
            where_conditions.append(f"l.league_code = '{league_filter}'")
        
        if season_filter:
            where_conditions.append(f"s.season_code = '{season_filter}'")
        
        where_clause = " AND ".join(where_conditions)
        
        # Spark SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð° Ð¿Ð¾Ð±ÐµÐ´
        spark_sql_query = f"""
        WITH team_matches AS (
            -- ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð¼Ð°Ñ‚Ñ‡Ð¸ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ (Ð´Ð¾Ð¼Ð° Ð¸ Ð² Ð³Ð¾ÑÑ‚ÑÑ…)
            SELECT 
                t.team_id,
                t.team_name,
                l.league_name,
                m.match_id,
                m.home_team_id,
                m.away_team_id,
                m.home_goals,
                m.away_goals
            FROM teams t
            JOIN matches m ON (t.team_id = m.home_team_id OR t.team_id = m.away_team_id)
            JOIN leagues l ON m.league_id = l.league_id
            JOIN seasons s ON m.season_id = s.season_id
            WHERE {where_clause}
        ),
        home_stats AS (
            -- Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð´Ð¾Ð¼Ð°ÑˆÐ½Ð¸Ñ… Ð¼Ð°Ñ‚Ñ‡ÐµÐ¹
            SELECT 
                team_id,
                team_name,
                league_name,
                COUNT(*) as home_matches,
                SUM(CASE WHEN home_team_id = team_id AND home_goals > away_goals THEN 1 ELSE 0 END) as home_wins
            FROM team_matches
            WHERE home_team_id = team_id
            GROUP BY team_id, team_name, league_name
        ),
        away_stats AS (
            -- Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð²Ñ‹ÐµÐ·Ð´Ð½Ñ‹Ñ… Ð¼Ð°Ñ‚Ñ‡ÐµÐ¹
            SELECT 
                team_id,
                COUNT(*) as away_matches,
                SUM(CASE WHEN away_team_id = team_id AND away_goals > home_goals THEN 1 ELSE 0 END) as away_wins
            FROM team_matches
            WHERE away_team_id = team_id
            GROUP BY team_id
        )
        -- Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð¾Ð¼ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð¾Ð²
        SELECT 
            h.team_id,
            h.team_name,
            h.league_name,
            h.home_matches,
            h.home_wins,
            ROUND((h.home_wins * 100.0) / NULLIF(h.home_matches, 0), 2) as home_win_pct,
            a.away_matches,
            a.away_wins,
            ROUND((a.away_wins * 100.0) / NULLIF(a.away_matches, 0), 2) as away_win_pct,
            ROUND(
                ((h.home_wins + a.away_wins) * 100.0) / NULLIF((h.home_matches + a.away_matches), 0), 
                2
            ) as total_win_pct
        FROM home_stats h
        JOIN away_stats a ON h.team_id = a.team_id
        ORDER BY total_win_pct DESC, home_win_pct DESC
        LIMIT {top_n}
        """
        
        logger.info("Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Spark SQL Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°...")
        logger.info(f"Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹: Ð»Ð¸Ð³Ð°={league_filter or 'Ð²ÑÐµ'}, ÑÐµÐ·Ð¾Ð½={season_filter or 'Ð²ÑÐµ'}")
        
        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ
        result_df = self.spark.sql(spark_sql_query)
        
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð² ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸
        logger.info(f"\nðŸ† Ð¢Ð¾Ð¿-{top_n} ÐºÐ¾Ð¼Ð°Ð½Ð´ Ð¿Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ñƒ Ð¿Ð¾Ð±ÐµÐ´:")
        result_df.show(truncate=False)
        
        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² pandas Ð´Ð»Ñ Ð´Ð°Ð»ÑŒÐ½ÐµÐ¹ÑˆÐµÐ¹ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        pandas_df = result_df.toPandas()
        
        logger.info(f"âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°. ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´: {len(pandas_df)}")
        
        return pandas_df
    
    def calculate_team_dynamics(
        self,
        league_filter: Optional[str] = None,
        season_filter: Optional[str] = None,
        team_names: Optional[List[str]] = None,
        output_parquet_path: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Ð—ÐÐ”ÐÐ§Ð 3: Ð”Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¿Ð¾ ÑÐµÐ·Ð¾Ð½Ð°Ð¼/Ð¼ÐµÑÑÑ†Ð°Ð¼
        
        Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ ÐºÑƒÐ¼ÑƒÐ»ÑÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ Ð¿Ð¾ Ñ…Ð¾Ð´Ñƒ ÑÐµÐ·Ð¾Ð½Ð°
        Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð¾ÐºÐ¾Ð½Ð½Ñ‹Ñ… Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ Spark.
        
        Args:
            league_filter: Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ ÐºÐ¾Ð´Ñƒ Ð»Ð¸Ð³Ð¸ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 'epl')
            season_filter: Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ ÑÐµÐ·Ð¾Ð½Ñƒ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, '2023-2024')
            team_names: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ð¹ ÐºÐ¾Ð¼Ð°Ð½Ð´ Ð´Ð»Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ (ÐµÑÐ»Ð¸ None â€” Ð²ÑÐµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹)
            output_parquet_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð° Ð² Parquet
        
        Returns:
            pd.DataFrame Ñ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ°Ð¼Ð¸:
                - team_id, team_name, league_name, season_code
                - match_date, match_number
                - points, goal_diff (Ð·Ð° Ð¼Ð°Ñ‚Ñ‡)
                - cumulative_points, cumulative_goal_diff (Ð½Ð°ÐºÐ¾Ð¿Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ)
                - goals_for, goals_against (Ð·Ð° Ð¼Ð°Ñ‚Ñ‡)
        """
        self.initialize_spark()
        
        logger.info("=" * 70)
        logger.info("Ð—ÐÐ”ÐÐ§Ð 3: Ð”Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð¿Ð¾ ÑÐµÐ·Ð¾Ð½Ð°Ð¼ (Spark Window Functions)")
        logger.info("=" * 70)
        
        # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¸Ð· PostgreSQL
        matches_df = self.read_table_from_postgres("matches")
        teams_df = self.read_table_from_postgres("teams")
        leagues_df = self.read_table_from_postgres("leagues")
        seasons_df = self.read_table_from_postgres("seasons")
        
        # Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð´Ð»Ñ Spark SQL
        matches_df.createOrReplaceTempView("matches")
        teams_df.createOrReplaceTempView("teams")
        leagues_df.createOrReplaceTempView("leagues")
        seasons_df.createOrReplaceTempView("seasons")
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ WHERE ÑƒÑÐ»Ð¾Ð²Ð¸Ñ
        where_conditions = ["m.home_goals IS NOT NULL", "m.away_goals IS NOT NULL"]
        
        if league_filter:
            where_conditions.append(f"l.league_code = '{league_filter}'")
        
        if season_filter:
            where_conditions.append(f"s.season_code = '{season_filter}'")
        
        where_clause = " AND ".join(where_conditions)
        
        # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°Ð¼ (ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½)
        team_filter_clause = ""
        if team_names:
            team_names_str = ", ".join([f"'{name}'" for name in team_names])
            team_filter_clause = f"AND t.team_name IN ({team_names_str})"
        
        logger.info(f"Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ñ‹: Ð»Ð¸Ð³Ð°={league_filter or 'Ð²ÑÐµ'}, ÑÐµÐ·Ð¾Ð½={season_filter or 'Ð²ÑÐµ'}")
        if team_names:
            logger.info(f"ÐšÐ¾Ð¼Ð°Ð½Ð´Ñ‹: {', '.join(team_names)}")
        
        # Spark SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ: Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð´Ð¾Ð¼Ð°ÑˆÐ½Ð¸Ðµ Ð¸ Ð³Ð¾ÑÑ‚ÐµÐ²Ñ‹Ðµ Ð¼Ð°Ñ‚Ñ‡Ð¸
        # Ð¸ Ñ€Ð°ÑÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹
        spark_sql_query = f"""
        WITH all_team_matches AS (
            -- Ð”Ð¾Ð¼Ð°ÑˆÐ½Ð¸Ðµ Ð¼Ð°Ñ‚Ñ‡Ð¸
            SELECT 
                t.team_id,
                t.team_name,
                l.league_id,
                l.league_name,
                l.league_code,
                s.season_id,
                s.season_code,
                m.match_id,
                m.match_date,
                m.home_goals as goals_for,
                m.away_goals as goals_against,
                CASE 
                    WHEN m.home_goals > m.away_goals THEN 3
                    WHEN m.home_goals = m.away_goals THEN 1
                    ELSE 0
                END as points,
                (m.home_goals - m.away_goals) as goal_diff,
                'home' as venue_type
            FROM matches m
            JOIN teams t ON m.home_team_id = t.team_id
            JOIN leagues l ON m.league_id = l.league_id
            JOIN seasons s ON m.season_id = s.season_id
            WHERE {where_clause} {team_filter_clause}
            
            UNION ALL
            
            -- Ð“Ð¾ÑÑ‚ÐµÐ²Ñ‹Ðµ Ð¼Ð°Ñ‚Ñ‡Ð¸
            SELECT 
                t.team_id,
                t.team_name,
                l.league_id,
                l.league_name,
                l.league_code,
                s.season_id,
                s.season_code,
                m.match_id,
                m.match_date,
                m.away_goals as goals_for,
                m.home_goals as goals_against,
                CASE 
                    WHEN m.away_goals > m.home_goals THEN 3
                    WHEN m.away_goals = m.home_goals THEN 1
                    ELSE 0
                END as points,
                (m.away_goals - m.home_goals) as goal_diff,
                'away' as venue_type
            FROM matches m
            JOIN teams t ON m.away_team_id = t.team_id
            JOIN leagues l ON m.league_id = l.league_id
            JOIN seasons s ON m.season_id = s.season_id
            WHERE {where_clause} {team_filter_clause}
        )
        SELECT 
            team_id,
            team_name,
            league_id,
            league_name,
            league_code,
            season_id,
            season_code,
            match_id,
            match_date,
            goals_for,
            goals_against,
            points,
            goal_diff,
            venue_type
        FROM all_team_matches
        ORDER BY team_id, season_id, match_date
        """
        
        logger.info("Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Spark SQL Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¼Ð°Ñ‚Ñ‡ÐµÐ¹...")
        base_df = self.spark.sql(spark_sql_query)
        
        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð¾ÐºÐ¾Ð½Ð½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð´Ð»Ñ ÐºÑƒÐ¼ÑƒÐ»ÑÑ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº
        logger.info("ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð¾ÐºÐ¾Ð½Ð½Ñ‹Ñ… Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ Ð´Ð»Ñ Ñ€Ð°ÑÑ‡ÐµÑ‚Ð° ÐºÑƒÐ¼ÑƒÐ»ÑÑ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº...")
        
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¾ÐºÐ½Ð¾: Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ ÐºÐ¾Ð¼Ð°Ð½Ð´Ðµ Ð¸ ÑÐµÐ·Ð¾Ð½Ñƒ, ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð¿Ð¾ Ð´Ð°Ñ‚Ðµ
        window_spec = Window.partitionBy("team_id", "season_id").orderBy("match_date")
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÑƒÐ¼ÑƒÐ»ÑÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
        result_df = base_df \
            .withColumn("match_number", F.row_number().over(window_spec)) \
            .withColumn("cumulative_points", F.sum("points").over(window_spec)) \
            .withColumn("cumulative_goal_diff", F.sum("goal_diff").over(window_spec)) \
            .withColumn("cumulative_goals_for", F.sum("goals_for").over(window_spec)) \
            .withColumn("cumulative_goals_against", F.sum("goals_against").over(window_spec))
        
        # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð² ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸
        logger.info(f"\nðŸ“Š ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ ÐºÐ¾Ð¼Ð°Ð½Ð´:")
        result_df.show(20, truncate=False)
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² Parquet ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½ Ð¿ÑƒÑ‚ÑŒ
        if output_parquet_path:
            parquet_path = Path(output_parquet_path)
            parquet_path.parent.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Parquet: {output_parquet_path}")
            result_df.write.mode("overwrite").parquet(str(parquet_path))
            logger.info(f"âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² Parquet: {output_parquet_path}")
        
        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² pandas Ð´Ð»Ñ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        pandas_df = result_df.toPandas()
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
        unique_teams = pandas_df['team_name'].nunique()
        unique_seasons = pandas_df['season_code'].nunique()
        total_records = len(pandas_df)
        
        logger.info(f"\nâœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°:")
        logger.info(f"   - ÐšÐ¾Ð¼Ð°Ð½Ð´: {unique_teams}")
        logger.info(f"   - Ð¡ÐµÐ·Ð¾Ð½Ð¾Ð²: {unique_seasons}")
        logger.info(f"   - Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {total_records}")
        
        return pandas_df
    
    def load_dynamics_from_parquet(self, parquet_path: str) -> pd.DataFrame:
        """
        Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸ Ð¸Ð· Parquet Ñ„Ð°Ð¹Ð»Ð°
        
        Args:
            parquet_path: ÐŸÑƒÑ‚ÑŒ Ðº Parquet Ñ„Ð°Ð¹Ð»Ñƒ
        
        Returns:
            pd.DataFrame Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸
        """
        self.initialize_spark()
        
        logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Parquet: {parquet_path}")
        
        df = self.spark.read.parquet(parquet_path)
        pandas_df = df.toPandas()
        
        logger.info(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(pandas_df)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
        
        return pandas_df
    
    def get_detailed_match_statistics(self) -> pd.DataFrame:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½ÑƒÑŽ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð¿Ð¾ Ð²ÑÐµÐ¼ Ð¼Ð°Ñ‚Ñ‡Ð°Ð¼
        
        Returns:
            pd.DataFrame: DataFrame Ñ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹
        """
        self.initialize_spark()
        
        query = """
        SELECT 
            m.match_id,
            m.match_date,
            l.league_name,
            s.season_code,
            ht.team_name as home_team,
            at.team_name as away_team,
            m.home_goals,
            m.away_goals,
            m.home_xg,
            m.away_xg,
            CASE 
                WHEN m.home_goals > m.away_goals THEN 'HOME_WIN'
                WHEN m.home_goals < m.away_goals THEN 'AWAY_WIN'
                ELSE 'DRAW'
            END as result
        FROM matches m
        JOIN teams ht ON m.home_team_id = ht.team_id
        JOIN teams at ON m.away_team_id = at.team_id
        JOIN leagues l ON m.league_id = l.league_id
        JOIN seasons s ON m.season_id = s.season_id
        WHERE m.home_goals IS NOT NULL
        ORDER BY m.match_date DESC
        """
        
        result_df = self.read_query_from_postgres(query)
        return result_df.toPandas()
    
    def close(self):
        """Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÑ‚ Spark ÑÐµÑÑÐ¸ÑŽ"""
        if self.spark is not None:
            logger.info("Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Spark ÑÐµÑÑÐ¸Ð¸...")
            self.spark_config.stop_spark_session()
            self.spark = None
            logger.info("âœ… Spark ÑÐµÑÑÐ¸Ñ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ð°")
    
    def __enter__(self):
        """Context manager: Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Spark Ð¿Ñ€Ð¸ Ð²Ñ…Ð¾Ð´Ðµ"""
        self.initialize_spark()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager: Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÑ‚ Spark Ð¿Ñ€Ð¸ Ð²Ñ‹Ñ…Ð¾Ð´Ðµ"""
        self.close()

